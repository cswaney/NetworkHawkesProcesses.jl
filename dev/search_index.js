var documenterSearchIndex = {"docs":
[{"location":"api/#Hawkes-Processes","page":"API","title":"Hawkes Processes","text":"","category":"section"},{"location":"api/#Continuous-Processes","page":"API","title":"Continuous Processes","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ContinuousStandardHawkesProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.ContinuousStandardHawkesProcess","page":"API","title":"NetworkHawkesProcesses.ContinuousStandardHawkesProcess","text":"ContinuousStandardHawkesProcess(baseline, impulses, weights)\n\nA continuous standard Hawkes processes.\n\nEquivalent to a continuous network Hawkes process with a fully-connected network.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"ContinuousNetworkHawkesProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.ContinuousNetworkHawkesProcess","page":"API","title":"NetworkHawkesProcesses.ContinuousNetworkHawkesProcess","text":"ContinuousNetworkHawkesProcess(baseline, impulses, weights, adjacency_matrix, network)\n\nA continuous network Hawkes process.\n\nNetwork Hawkes processes allow for partially-connected networks. The binary adjacency_matrix defines network connections generated by the probabilistic network model, where adjacency_matrix[i, j] represents a connection from node i to node j.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"rand(process::ContinuousHawkesProcess, duration::Float64)","category":"page"},{"location":"api/#Base.rand-Tuple{ContinuousHawkesProcess, Float64}","page":"API","title":"Base.rand","text":"rand(process::ContinuousHawkesProcess, duration)\n\nSample a random sequence of events from a continuous Hawkes process.\n\nArguments\n\nduration::Float64: the sample duration.\n\nReturns\n\ndata::Tuple{Vector{Float64},Vector{Int64},Float64}: a tuple containing a vector of events, a vector of nodes associated with each event, and the duration of the data sample.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"loglikelihood(process::ContinuousHawkesProcess, data)","category":"page"},{"location":"api/#NetworkHawkesProcesses.loglikelihood-Tuple{ContinuousHawkesProcess, Any}","page":"API","title":"NetworkHawkesProcesses.loglikelihood","text":"loglikelihood(process::ContinuousHawkesProcess, data)\n\nCalculate the log-likelihood of data.\n\nArguments\n\ndata::Tuple{Vector{Float64},Vector{Int64},Float64}: a tuple containing a vector of events, a vector of nodes associated with each event, and the duration of the data sample.\nrecursive::Bool: use recursive formulation, if possible.\n\nReturns\n\nll::Float64: the log-likelihood of the data.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"intensity(process::ContinuousHawkesProcess, data, times::Vector{Float64})","category":"page"},{"location":"api/#NetworkHawkesProcesses.intensity-Tuple{ContinuousHawkesProcess, Any, Vector{Float64}}","page":"API","title":"NetworkHawkesProcesses.intensity","text":"intensity(process::ContinuousHawkesProcess, data, times)\n\nCalculate the intensity of process at times given data.\n\nArguments\n\ndata::Tuple{Vector{Float64},Vector{Int64},Float64}: a tuple containing a vector of events, a vector of nodes associated with each event, and the duration of the data sample.\ntimes::Vector{Float64}: a vector times where the process intensity will be computed.\n\nReturns\n\nλ::Vector{Float64}: a len(times) array of intensities conditional on data and process.\n\n\n\n\n\n","category":"method"},{"location":"api/#Discrete-Processes","page":"API","title":"Discrete Processes","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DiscreteStandardHawkesProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.DiscreteStandardHawkesProcess","page":"API","title":"NetworkHawkesProcesses.DiscreteStandardHawkesProcess","text":"DiscreteStandardHawkesProcess(baseline, impulses, weights, dt)\n\nA standard discrete Hawkes processes with time step dt.\n\nEquivalent to a discrete network Hawkes process with a fully-connected network.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"DiscreteNetworkHawkesProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.DiscreteNetworkHawkesProcess","page":"API","title":"NetworkHawkesProcesses.DiscreteNetworkHawkesProcess","text":"DiscreteNetworkHawkesProcess(baseline, impulses, weights, adjacency_matrix, network, dt)\n\nA discrete network Hawkes process with time step dt.\n\nNetwork Hawkes processes allow for partially-connected networks. The binary adjacency_matrix defines network connections generated by the probabilistic network model, where adjacency_matrix[i, j] represents a connection from node i to node j.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"rand(process::DiscreteHawkesProcess, steps::Int64)","category":"page"},{"location":"api/#Base.rand-Tuple{DiscreteHawkesProcess, Int64}","page":"API","title":"Base.rand","text":"rand(p::DiscreteHawkesProcess, steps)\n\nSample a random sequence of events from a discrete Hawkes process.\n\nArguments\n\nT::Int64: the number of time steps to sample.\n\nReturns\n\nS::Array{Int64,2}: an N x T array of event counts.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"loglikelihood(process::DiscreteHawkesProcess, data)","category":"page"},{"location":"api/#NetworkHawkesProcesses.loglikelihood-Tuple{DiscreteHawkesProcess, Any}","page":"API","title":"NetworkHawkesProcesses.loglikelihood","text":"loglikelihood(process::DiscreteHawkesProcess, data)\nloglikelihood(process::DiscreteHawkesProcess, data, convolved)\n\nCalculate the log-likelihood of data. Providing convolved skips computation of the convolution step.\n\nArguments\n\ndata::Array{Int64,2}: N x T array of event counts.\nconvolved::Array{Float64,3}: T x N x B array of event counts convolved with basis functions.\n\nReturns\n\nll::Float64: the log-likelihood of the event counts.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"intensity(process::DiscreteHawkesProcess, convolved)","category":"page"},{"location":"api/#NetworkHawkesProcesses.intensity-Tuple{DiscreteHawkesProcess, Any}","page":"API","title":"NetworkHawkesProcesses.intensity","text":"intensity(process::DiscreteHawkesProcess, convolved)\n\nCalculate the intensity at time all times t ∈ [1, 2, ..., T] given pre-convolved event data.\n\nArguments\n\nconvolved::Array{Float64,3}: T x N x B array of event counts convolved with basis functions.\n\nReturns\n\nλ::Array{Float64,2}: a T x N array of intensities conditional on the convolved event counts.\n\n\n\n\n\n","category":"method"},{"location":"api/#Baseline-Processes","page":"API","title":"Baseline Processes","text":"","category":"section"},{"location":"api/#Continuous-Processes-2","page":"API","title":"Continuous Processes","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"HomogeneousProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.HomogeneousProcess","page":"API","title":"NetworkHawkesProcesses.HomogeneousProcess","text":"HomogeneousProcess\n\nA homogeneous Poisson process with constant intensity λ ~ Gamma(α0, β0).\n\nArguments\n\nλ: constant intensity parameter.\nα0: shape parameter of Gamma prior for Bayesian inference (default: 1.0).\nβ0: rate parameter of Gamma prior for Bayesian inference (default: 1.0).\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"LogGaussianCoxProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.LogGaussianCoxProcess","page":"API","title":"NetworkHawkesProcesses.LogGaussianCoxProcess","text":"LogGaussianCoxProcess\n\nA log Gaussian Cox process constructed from a realization of a Gaussian process at fixed gridpoints.\n\nThe model is\n\ny ~ GP(0, K)\nλ(t) = exp(m + y(t))\ns ~ PP(λ(t))\n\nFor an arbitrary set of gridpoints, x[1], ..., x[N], a corresponding sample of the Gaussian process, y[1], ..., y[N], has a N(0, Σ) distribution, where\n\nΣ[i, j] = K(x[i], x[j])\n\nThe process is sampled by interpolating between intensity values λ[1], ..., λ[N].\n\nArguments\n\nx::Vector{Float64}: a strictly increasing vectors of sampling grid points starting from x[1] = 0.0.\nλ::Vector{Vector{Float64}}: a list of non-negative intensity vectors such that λ[k][i] = λ[k]([x[i]).\nΣ::PDMat{Float64}: a positive-definite variance matrix.\nm::Vector{Float64}: intensity offsets equal to log(λ0) of homogeneous processes.\n\n\n\n\n\n","category":"type"},{"location":"api/#Discrete-Processes-2","page":"API","title":"Discrete Processes","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DiscreteHomogeneousProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.DiscreteHomogeneousProcess","page":"API","title":"NetworkHawkesProcesses.DiscreteHomogeneousProcess","text":"DiscreteHomogeneousProcess\n\nA discrete, homogeneous Poisson process.\n\nThe model supports Bayesian inference of the probabilistic model:\n\nλ[i] ~ Gamma(λ[i] | α0, β0) (i = 1, ..., N)\nx[i, t] ~ Poisson(x[i, t] | λ[i] * dt) (t = 1, ..., T)\n\nArguments\n\nλ::Vector{Float64}: a vector of intensities.\nα0::Float64: the shape parameter of the Gamma prior.\nβ0::Float64: the inverse-scale (i.e., rate) parameter of the Gamma prior.\ndt::Float64: the physical time represented by each time step, t.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"DiscreteLogGaussianCoxProcess","category":"page"},{"location":"api/#NetworkHawkesProcesses.DiscreteLogGaussianCoxProcess","page":"API","title":"NetworkHawkesProcesses.DiscreteLogGaussianCoxProcess","text":"DiscreteLogGaussianCoxProcess\n\nA discrete log Gaussian Cox process constructed from a realization of a Gaussian process at fixed gridpoints.\n\nThe probabilistic model is:\n\ny ~ GP(0, K)\nλ[t] = exp(m + y[t])\ns ~ PP(λ[t])\n\nFor an arbitrary set of gridpoints, x[1] = 1, ..., x[N] = T, a corresponding sample of the Gaussian process, y[1], ..., y[N], has a N(0, Σ) distribution, where\n\nΣ[i, j] = K(x[i], x[j])\n\nThe process is sampled by interpolating between intensity values λ[1], ..., λ[N].\n\nArguments\n\nx::Vector{Int64}: a strictly increasing vector of sampling gridpoints.\nλ::Matrix{Float64}: a non-negative, T x N intensity matrix, ie, λ[i, n] = λ_n([x[i]).\nΣ::Matrix{Float64}: a positive-definite variance matrix.\nm::Float64: intensity offset equal to log(λ0) of a homogeneous process.\n\n\n\n\n\n","category":"type"},{"location":"api/#Impulse-Response-Models","page":"API","title":"Impulse Response Models","text":"","category":"section"},{"location":"api/#Continuous-Models","page":"API","title":"Continuous Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"ExponentialImpulseResponse","category":"page"},{"location":"api/#NetworkHawkesProcesses.ExponentialImpulseResponse","page":"API","title":"NetworkHawkesProcesses.ExponentialImpulseResponse","text":"ExponentialImpulseResponse\n\nAn exponential impulse response function with Gamma prior.\n\nThis model is a building block for Hawkes processes. Given a number of child events on node j attributed to node i, it generates event times according to a exponential distribution with rate parameter θ[i, j].\n\nFor Bayesian inference we assume a uniform Gamma priors for θ:\n\n`θ[i, j] ~ Gamma(κ, ν) for all i, j`\n\nThe resulting model is only conjugate in the limit as the sampling duration approaches infinity, however. Thus, Bayesian inference for the exponential process is approximate.\n\nArguments\n\nθ::Float64: the rate of the exponential distribution (i.e., 1 / scale).\nα: shape parameter of Gamma prior for θ (default: 1.0).\nβ: rate parameter of Gamma prior for θ (default: 1.0).\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"LogitNormalImpulseResponse","category":"page"},{"location":"api/#NetworkHawkesProcesses.LogitNormalImpulseResponse","page":"API","title":"NetworkHawkesProcesses.LogitNormalImpulseResponse","text":"LogitNormalImpulseResponse\n\nA logit-normal impulse response function.\n\nThis model is a building block for Hawkes process. Given a number of child events on node j attributed to node i, it generates event times according to a stretched logit-normal distribution with location parameter μ[i, j], precision parameter τ[i, j], and support [0, Δtmax].\n\nFor Bayesian inference we assume a uniform normal-gamma prior for μ and τ:\n\n`τ[i, j] ~ Gamma(α0, β0)` for all i, j\n`μ[i, j] | σ[i, j] ~ Normal(μ[i, j], σ[i, j])` for all i, j\n\nwhere σ[i,, j] = 1 / sqrt(κ[i, j] * τ[i, j]) for all i, j.\n\nArguments\n\nμ::Float64: the location of the logit-normal distribution.\nτ::Float64: the precision of the logit-normal distribution (i.e., 1 / σ^2).\nμμ::Float64: the location of the normal prior for μ (default: 1.0).\nκμ::Float64: the precision multiplier of the normal prior for μ (default: 1.0).\nα: shape parameter of gamma prior for τ (default: 1.0).\nβ: rate parameter of gamma prior for τ (default: 1.0).\nΔtmax::Float64: the upperbound of the process support, [0, Δtmax].\n\n\n\n\n\n","category":"type"},{"location":"api/#Discrete-Models","page":"API","title":"Discrete Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DiscreteGaussianImpulseResponse","category":"page"},{"location":"api/#NetworkHawkesProcesses.DiscreteGaussianImpulseResponse","page":"API","title":"NetworkHawkesProcesses.DiscreteGaussianImpulseResponse","text":"DiscreteGaussianImpulseResponse <: DiscreteImpulseResponse\n\nIf there are fewer basis functions than lags, then the means are evenly spaced between the endpoints of [1, L] such that the distance to the nearest mean or endpoint is the same everywhere. As a result, you can only have a means located at the endpoints if the number of basis functions is at least as great as the number of lags.\n\n\n\n\n\n","category":"type"},{"location":"api/#Weight-Models","page":"API","title":"Weight Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DenseWeightModel","category":"page"},{"location":"api/#NetworkHawkesProcesses.DenseWeightModel","page":"API","title":"NetworkHawkesProcesses.DenseWeightModel","text":"DenseWeightModel\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"SparseWeightModel","category":"page"},{"location":"api/#NetworkHawkesProcesses.SparseWeightModel","page":"API","title":"NetworkHawkesProcesses.SparseWeightModel","text":"SparseWeightModel\n\n\n\n\n\n","category":"type"},{"location":"api/#Network-Models","page":"API","title":"Network Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DenseNetworkModel","category":"page"},{"location":"api/#NetworkHawkesProcesses.DenseNetworkModel","page":"API","title":"NetworkHawkesProcesses.DenseNetworkModel","text":"DenseNetwork\n\nA fully-connected network model.\n\nArguments\n\nnnodes::Int64: the network size.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"BernoulliNetworkModel","category":"page"},{"location":"api/#NetworkHawkesProcesses.BernoulliNetworkModel","page":"API","title":"NetworkHawkesProcesses.BernoulliNetworkModel","text":"BernoulliNetworkModel\n\nA network model with independent Bernoulli distributed link probabilities.\n\nArguments\n\nρ::Float64: the probability of a connection between any two nodes (ρ ∈ [0., 1.]).\n...\nN::Int64: the network size / number of nodes.\n\n\n\n\n\n","category":"type"},{"location":"api/#Inference","page":"API","title":"Inference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"mcmc!(process::HawkesProcess, data; nsteps=1000, verbose=false)","category":"page"},{"location":"api/#NetworkHawkesProcesses.mcmc!-Tuple{HawkesProcess, Any}","page":"API","title":"NetworkHawkesProcesses.mcmc!","text":"mcmc!(process::HawkesProcess, data; kwargs...)\n\nPerform Markov chain Monte Carlo (Gibbs) sampling.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"vb!(process::HawkesProcess, data; max_steps::Int64=1_000, Δx_thresh=1e-6, Δq_thresh=1e-2)","category":"page"},{"location":"api/#NetworkHawkesProcesses.vb!-Tuple{HawkesProcess, Any}","page":"API","title":"NetworkHawkesProcesses.vb!","text":"vb!(process::DiscreteHawkesProcess, data; kwargs...)\n\nPerform mean-field variational inference.\n\nThe variational distribution takes the form q(λ0)q(θ)q(W)q(A)q(ω), where:\n\nq(λ0) = Gamma(α, β)\nq(θ) = Dir(γ)\nq(W) = Gamma(kappa , ν)\nq(A) = Bern(ρ)\nq(ω) = Mult(u)\n\nArguments\n\n\n\n\nKeyword Arguments\n\nmax_steps::Int64: maximum number of updates to perform.\nΔx_thresh::Float64: ?\nΔq_thresh::Float64: ?\n\nReturn\n\nres::: a struct containing results of the inference routine\n\n\n\n\n\n","category":"method"},{"location":"examples/","page":"Examples","title":"Examples","text":"Coming soon!","category":"page"},{"location":"#NetworkHawkesProcesses.jl","page":"Home","title":"NetworkHawkesProcesses.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NetworkHawkesProcess.jl implements a class of probabilistic models that combine multivariate Hawkes processes with network models (Linderman, 2016). The intensity of such models takes the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"lambda_n(t) = lambda_n^(0)(t) + sum_n sum_substackc_i = n s_i  t a_n rightarrow  h_n rightarrow n(t - s_i    theta_n rightarrow n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where a_n rightarrow n is a binary variable representing the existence of a directed link from process n to process n generated by a process","category":"page"},{"location":"","page":"Home","title":"Home","text":"a_n rightarrow n sim p(a_n rightarrow n    eta_n rightarrow n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"These models–network Hawkes processes–permit simultaneous inference on the structure of a network and its event generating process. The NetworkHawkesProcesses package provides methods to simulate and estimate such processes. It allows researchers to construct models from a flexible set of model components, run inference from a list of compatible methods (including maximum-likelihood estimation, Markov chain Monte Carlo sampling, and variational inference), and explore results with visualization and diagnostic utilities.","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Supports continuous and discrete processes\nUses modular design to support extensible components\nImplements simulation via Poisson thinning\nProvides multiple estimation/inference methods\nSupports a wide range of network specifications\nSupports non-homogeneous baselines\nAccelerates methods via Julia's built-in Threads module","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg;\njulia> Pkg.add(\"https://github.com/cswaney/NetworkHawkesProcesses.jl.git\")\n# or pkg> add \"https://github.com/cswaney/NetworkHawkesProcesses.jl.git\"","category":"page"},{"location":"#Creating-Processes","page":"Home","title":"Creating Processes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Proceses are constructed from a combination of component models. All processes require a Baseline model, and ImpulseResponse model, and a Weights model. For a ContinuousStandardHawkesProces, which does not incorporate network structe, these are all that is needed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using NetworkHawkesProcesses\n\nnnodes = 2\nweight = 0.1\nΔtmax = 1.0\n\nbaseline = NetworkHawkesProcesses.HomogeneousProcess(ones(nnodes))\nweights = NetworkHawkesProcesses.DenseWeightModel(weight .* ones(nnodes, nnodes))\nimpulses = NetworkHawkesProcesses.ExponentialImpulseResponse(ones(nnodes, nnodes))\nprocess = NetworkHawkesProcesses.ContinuousStandardHawkesProcess(baseline, impulses, weights)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A ContinuousNetworkHawkesProces requires a Network model and adjacency matrix as well:","category":"page"},{"location":"","page":"Home","title":"Home","text":"network = NetworkHawkesProcesses.BernoulliNetworkModel(0.5, nnodes)\nlinks = NetworkHawkesProcesses.rand(network)\nprocess = NetworkHawkesProcesses.ContinuousNetworkHawkesProcess(baseline, impulses, weights, links, network)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Discrete processes are constructed in a similar fashion, but some component models must be changed to an appropriate discrete version and a time step size is required. For example, here is how to construct a DiscreteStandardHawkesProcess:","category":"page"},{"location":"","page":"Home","title":"Home","text":"nnodes = 2\nnbasis = 3\nnlags = 4\nweight = 0.1\ndt = 1.0\n\nbaseline = NetworkHawkesProcesses.DiscreteHomogeneousProcess(ones(nnodes), dt)\nweights = NetworkHawkesProcesses.DenseWeightModel(weight .* ones(nnodes, nnodes))\nimpulses = NetworkHawkesProcesses.DiscreteGaussianImpulseResponse(ones(nnodes, nnodes, nbasis) ./ nbasis, nlags, dt)\nprocess = NetworkHawkesProcesses.DiscreteStandardHawkesProcess(baseline, impulses, weights, dt)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A DiscreteNetworkHawkesProcess only requires the addition of a Network model and adjacency matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"network = NetworkHawkesProcesses.BernoulliNetworkModel(0.5, nnodes)\nlinks = NetworkHawkesProcesses.rand(network)\nprocess = NetworkHawkesProcesses.DiscreteNetworkHawkesProcess(baseline, impulses, weights, links, network, dt)","category":"page"},{"location":"#Simulating-Data","page":"Home","title":"Simulating Data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Following conventional (e.g., Disctributions), NetworkHawkesProcesses defines rand methods for the purpose of simulating data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"events, nodes, duration = rand(process, 100.0) # typeof(process) == ContinuousHawkesProcess\ndata = rand(process, 100) # typeof(process) == DiscreteHawkesProcess","category":"page"},{"location":"","page":"Home","title":"Home","text":"Generally, the simulation duration is any positive value, but some baselines (e.g., LogGaussianCoxProcess) define a maximum duration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that the format of simulated data is quite different for continuous and discrete processes. For continuous processes, simulation returns separate arrays of event times and nodes, in addition to the simulation duration. For discrete processes, simulated data is represented as a matrix whose rows contain event counts for each node, with each column representing a time step.","category":"page"},{"location":"","page":"Home","title":"Home","text":"size(data) # (2, 100)","category":"page"},{"location":"#Running-Inference","page":"Home","title":"Running Inference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package provides several method for performing inference, including maximum-likelihood estimation (mle!), Markov Chain Monte Carlo sampling (mcmc!), and variational inference (vb!, svi!).","category":"page"},{"location":"","page":"Home","title":"Home","text":"res = NetworkHawkesProcesses.mle!(process, data; verbose=true, regularize=true) # maximum a posteriori","category":"page"},{"location":"","page":"Home","title":"Home","text":"The inference methods available depend on the type of model (see the table below). Processes with Network models don't provide maximum-likelihood estimation due to the presence on binary parameters in the adjacency matrix; and only discrete processes provide variational inference methods.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Model Available Methods\nContinuousStandardHawkesProcess mle!, mcmc!\nContinuousNetworkHawkesProcess mcmc!\nDiscreteStandardHawkesProcess mle!, mcmc!, vb!, svi!\nDiscreteNetworkHawkesProcess mcmc!, vb!, svi!","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nAll inference methods overwrite model parameters. In addition, each method takes a default initial guess at the model parameters if no initial guess is provided.","category":"page"},{"location":"","page":"Home","title":"Home","text":"tip: Tip\nA fast option for an initial guess is to estimate a homogenous Poisson process (although this doesn't provide any guidance for impulse response, network, or connection parameters).","category":"page"},{"location":"#Reporting-Diagnostics","page":"Home","title":"Reporting Diagnostics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package provides several methods to examine models and simulated data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"isstable(processs) checks the stability of a Hawkes process. Unstable processes will often fail to generate finite data samples.\nintensity(process, data, times) calculates the intensity of a process (given data) for all times in times.\nloglikelihood(process, data) calculates the log-likelihood of a dataset.","category":"page"},{"location":"#multi-threading","page":"Home","title":"multi-threading","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Several methods use multi-threading to speed up computation. For all such methods, multi-threading is automatically activated and uses as many threads as available to the Julia process. For example, to use all local CPU threads (i.e., \"logical cores\"), start the Julia session with julia --threads auto.","category":"page"},{"location":"#Next-Steps","page":"Home","title":"Next Steps","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Check out our tutorials and examples to learn more about how the package works and see practical examples of its usage, or browse our API documentation for detailed information on package components.","category":"page"},{"location":"tutorial/#Mathematical-Background","page":"Tutorial","title":"Mathematical Background","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Hawkes processes are a class of inhomogeneous, autoregressive Poisson processes used to model the arrival of events that demonstrate self-exciting behavior, e.g., seismic activity. The intensity of a univariate Hawkes process given a sequence of events  s_i _i=1^N is given by the expression","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda(t) = lambda^(0)(t) + sum_s_i  t h(t - s_i    theta)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where lambda^(0) is the (possibly inhomogenous) baseline intensity of the process in the absence of events, and h(Delta t) is a non-negative impulse response that captures the increase in intensity due to each event. Simply put, every event arrival temporarily increases the likelihood of future events.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Multivariate Hawkes processes introduce the possibility of interactions between processes. The intensity of the n-th process given a sequence of events  s_i _i=1^N occurring on corresponding processes  c_i _i=1^N is given by","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda_n(t) = lambda_n^(0)(t) + sum_m sum_substackc_i = m s_i  t h_m rightarrow n(t - s_i    theta_m rightarrow n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where the impulse response h(Delta t    theta_m rightarrow n) now depends on the \"parent\" process, m, and \"child\" process, n.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NetworkHawkesProcess.jl implements a class of probabilistic models that combines multivariate Hawkes processes with network models (Linderman, 2016). The intensity of such models takes the form","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda_n(t) = lambda_n^(0)(t) + sum_m sum_substackc_i = m s_i  t a_m rightarrow n h_m rightarrow n(t - s_i    theta_m rightarrow n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where a_m rightarrow n is a binary variable representing the existence of a directed link from process m to process n generated by a process","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"a_m rightarrow n sim p(a_m rightarrow n    eta_m rightarrow n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"These models–network Hawkes processes–permit simultaneous inference on the structure of a network and its event generating process. The following sections demonstrate how to use this package to construct, simulate, and perform inference on such processes.","category":"page"},{"location":"tutorial/#Continuous-Processes","page":"Tutorial","title":"Continuous Processes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This section explores the continuous-time network Hawkes process. We'll examine the component parts of the process, generate simulated data, and learn how inference works. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's start by reviewing the multivariate, continuous-time network Hawkes process. The process consists of n Poisson processes, with the intensity of its n-th node taking the form","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda_n(t    s_m c_m ) = lambda_n^(0)(t) + sum_s_m  t a_c_m rightarrow n cdot w_c_m rightarrow n cdot hbar(Delta t theta_c_m rightarrow n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda_n^(0) is the baseline intensity of the process, which represents the intensity of the process in the abscence of events.\na_c_m rightarrow n is the (c_m n)-th component of the binary adjacency matrix, mathbfA. The components of mathbfA determine whether connections exists between nodes in the network. If no connection exists between nodes i and j, then events on node i have no effect on the intensity of node j. The model assumes that mathbfA is generated by a network model, as described below.\nw_c_m rightarrow n is the (c_m n)-th component of the weight matrix, mathbfW. Whereas a_i j determines the existence of a connection from node i to node j, w_i j determines the connection's strength.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nIn generally, the weight matrix can also be modeled as the result of a network-type model, but this option is not currently supported.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hbar(Delta t  theta_c_m rightarrow n) is a non-negative impulse response function, which controls the evolution of the n-th node's response to events on node c_m. Its shape is determined by connection-specific parameters theta_c_m rightarrow n. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In summary, the continuous-time network Hawkes process is a network of interacting Poisson processes, which consists of: a baseline, an adjacency matrix, a weight matrix, and an impulse response function. Now let's create some of these processes in NetworkHawkesProcesses to see how this all works.","category":"page"},{"location":"tutorial/#Standard-Hawkes-Processes","page":"Tutorial","title":"Standard Hawkes Processes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NetworkHawkesProcesses allows users to contruct models with network structure, but let's start by considering a simpler, \"standard\" Hawkes model, which we construct as follows:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nnodes = 2;\nbaseline = HomogeneousProcess(rand(nnodes));\nimpulses = ExponentialImpulseResponse(rand(nnodes, nnodes));\nweights = DenseWeightModel(rand(nnodes, nnodes));\nprocess = ContinuousStandardHawkesProcess(baseline, impulses, weights);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The process is straight-forward: define the component parts, then define the process. But let's take a look at the component parts to understand what we've just created.","category":"page"},{"location":"tutorial/#Baseline","page":"Tutorial","title":"Baseline","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"First, we need to define a baseline intensity. The simplest choice is a homogeneous Poisson process:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"lambda_n^(0)(t) = lambda_n^(0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We can create this process using HomogeneousProcess:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nnodes = 2;\nbaseline = HomogeneousProcess(rand(nnodes))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"However, a constant baseline intensity is unrealistic in many applications. The package also provides a flexible alternative, the LogGaussianCoxProcess. This process uses Gaussian processes to capture time-varying intensity, which is particularly useful when modeling processes with (e.g.) known periodic properties. We'll consider the LogGaussianCoxProcess more closely later.","category":"page"},{"location":"tutorial/#Weights","page":"Tutorial","title":"Weights","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Next, we need to define our weight model. In general, a weight model can be any network model that generates a non-negative weight matrix. Currently, the package only provides a simple weight model that represents weights as a constant matrix:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"weights = DenseWeightModel(rand(nnodes, nnodes))","category":"page"},{"location":"tutorial/#Impulse-Response","page":"Tutorial","title":"Impulse Response","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, we need to define an impulse response. Constraining hbar to be a valid probability density ensures that it only determines the timing of responses because each impulse response integrates to one, and therefore contributes identically to the likelihood of events. Additionally, each w_n rightarrow n becomes the expected number of events on node n resulting directly from a single event on node n. In other words, we can meaningfully compare weights.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's start with a classic choice, the exponential distribution:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hbar(Delta t theta_n rightarrow n) = theta_n rightarrow n exp -theta_n rightarrow n cdot Delta t ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Exponential Impulse Response)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We create this model with the ExponentialImpulseResponse struct:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"impulses = ExponentialImpulseResponse(rand(nnodes, nnodes))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In addition to specifying the actual impulse response function, this model defines data and logic used for inference. We'll skip these details for now.","category":"page"},{"location":"tutorial/#Stability","page":"Tutorial","title":"Stability","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Before we move on, we should note that we have assigned random values to our model parameters. While these values are valid for the component models, they may lead to an unstable process, i.e., whose intensity eventually \"blows up\". Let's check if our process is stable:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"isstable(process)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This function simply checks the stability condition:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"max_ lambda  texteig left( mathbfA odot mathbfW right)  10","category":"page"},{"location":"tutorial/#Simulation","page":"Tutorial","title":"Simulation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Next, let's examine what data generated by this process looks like. Following Julia convention, random data is generated using the rand method. For example, here's how to simulate data for a period of length T=10.0:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"events, nodes, duration = rand(process, 1000.0)\nprintln(\"(process generated $(length(events)) events)\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that the output of a continuous process has three parts: a list of event times (events), followed by a list of corresponding event nodes, and the duration of the simulation. The reason we return duration is that it is required for continuous-time inference methods. Returning it means that we run inference directly on the output of rand, i.e., without unpacking as above.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's take a look at our data:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"data = (events, nodes, duration)\nplot(processs, data)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Intensity)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Each point of the plot represents an event, and the curves represent the intensity of the nodes. The exponential impulse reponse function produces an instantaneous increase in intensity after each event, which is followed by a gradual decline. This puncuated behavior is undesirable or unrealistic in many applications. For that reason, NetworkHawkesProcesses provides a more flexible impulse resopnse function based on the logit-normal distribution:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hbar(Delta t mu_n rightarrow n tau_n rightarrow n) = frac1Z exp Bigg  frac-tau_n rightarrow n2 left( sigma^-1 left( fracDelta tDelta t_textmax right) right) Bigg ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hbar is again a proper probability distribution on 0 Delta t_textmax, where Delta t_textmax is a hyperparameter (i.e., not learned through inference). The distribution provides flexibility in the timing of the impulse response at the cost of a second parameter, mathbftau, as demonstrated in the figure below.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Logit-Normal Impulse Response)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"LogitNormalImpulseResponse implements this impulse reponse function along with all methods required to perform statistical inference. Let's update our process to use this impulse response and generate a new sample:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Δtmax = 1.0;\nimpulses = LogitNormalImpulseResponse(rand(nnodes, nnodes), rand(nnodes, nnodes), Δtmax);\nprocess = ContinuousStandardHawkesProcess(baseline, impulses, weights);\nevents, nodes, duration = data = rand(process, duration);\nprintln(\"(process generated $(length(events)) events)\")","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As demostrated in the following figure, the response to events is now noticably delayed:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Logit-Normal Impulse Response)","category":"page"},{"location":"tutorial/#Inference","page":"Tutorial","title":"Inference","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Typically, our interest is in learning the parameters of a model given data collected from an experiment or real-world dataset. ContinuousStandardHawkesProcess is compatible with maximum-likelihood estimation (mle!) and Markov chain Monte Carlo (Gibbs) sampling (mcmc!) methods:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using NetworkHawkesProcesses: params\norginial_process = deepcopy(process)\nmle!(process, data; verbose=true, regularize=true); # regularize w/ prior\n[params(process) params(original_process)]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Note that inference methods in NetworkHawkesProcesses are mutating operations. That is, they update the parameters of the process they are called on. In this case, we saved a copy of the original process for comparison with our inference results, but we wouldn't normally need to make a copy. Notice as well that we did not initialize these methods. Behind the scenes, mle! and mcmc! created default, random initial guesses of the model parameters. You can override the default value by providing a guess parameter. For example, we could use the results of maximum-likelihood estimation as initial parameter values for Gibbs sampling.","category":"page"},{"location":"tutorial/#Networks","page":"Tutorial","title":"Networks","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"At this point, our model allows us to infer the strength of connections between nodes, but not whether such connections actually exist. A simple approach to predicting connections is to apply a hard threshold to estimates of mathbfW, i.e., hatw_m rightarrow n leftarrow min left( hatw_m rightarrow n - w_textmin 0 right). The network Hawkes model provides a more disciplined approach by specifying a generative model for network connections.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In general, a network model takes the form","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p(mathbfA    mathbfz mathbfnu) = prod_n=1^N prod_n=1^N p(a_n rightarrow n    z_n z_n mathbfnu) = prod_n=1^N prod_n=1^N textBern(a_n rightarrow n    rho_n rightarrow n)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where rho_n rightarrow n = f(a_n z_n z_n nu). In the probabilistic modeling lingo, z_i are referred to as \"local\" parameters and nu is called a \"global\" parameter. NetworkHawkesProcesses has one such model built-in, the so-called Bernoulli network,","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p left( a_n rightarrow n right) = textBern left( a_n rightarrow n    rho right)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"which is implemented by the BernoulliNetworkModel struct. Constructing a continuous-time Hawkes process with this network model imposed on interactions is straight-forward:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"baseline = HomogeneousProcess([1.0, 2.0]);\nweights = DenseWeightModel([0.1 0.2; 0.2 0.1]);\nimpulses = LogitNormalImpulseResponse(ones(nnodes, nnodes), ones(nnodes, nnodes), Δtmax);\nnetwork = BernoulliNetworkModel(0.5, nnodes);\nlinks = [1 0; 1 1];\nprocess = ContinuousNetworkHawkesProcess(baseline, impulses, weights, links, network);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The steps are similar to those used to create a standard Hawkes process. We pass the baseline, weights, and impulse response models to ContinuousNetworkHawkesProcess as before, but we now also include the network model and binary connection matrix, links, which takes the same form as a realization of network model (we could have also done links = rand(network)).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Simulating data works exactly as before:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"data = rand(process, duration);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Maximum-likelihood estimation isn't permitted for network Hawkes processes due to the binary adjacency matrix. However, we can still perform Gibbs sampling:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"res = mcmc!(process, data; verbose=true);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, mcmc! resamples the process parameters nsteps=1000 times, and the samples are stored in res.samples. For example, we can get point estimates of model parameters by taking statistics of the samples:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Statistics\nmean(res.samples)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"note: Note\nTaking the median sample is the more typical choice, but median doesn't work on vectors, unfortunately. Instead, we can take medians of individual elements with median([x[1] for x in res.samples]).","category":"page"},{"location":"tutorial/#Discrete-Processes","page":"Tutorial","title":"Discrete Processes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Discrete-time processes are similar to their continuous-time counterparts in most respects. Essentially, model components are discretized in the time dimension, where applicable. This means that baselines become discrete-time Poisson processes and impulse responses are translated to discrete distributions, but there is no change to network models or weights.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"What might we want to use a discrete-time model? The primary motivation for discrete-time processes is faster inference. When events occur frequently, discrete-time processes offer faster inference because several events fall into the same time bin. Thus, discrete-time inference scales linearly with number of time bins. In contrast, continuous-time models scale quadratically with the number of events (although this is mollified by introduction of maximal look-back periods)[1]. In addition, the discrete-time model permits variational inference, which leads to further computational gains (at the cost of simlifying dependence structures). Furthermore, in many cases we may only have access to discrete data, which necessicates a discrete-time model.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"[1]: On the other hand, discrete-time processes offer lower resolution: they cannot capture \"instantaneous\" interactions between events. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here's how we would implement the previous example using a discrete-time model instead:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"nnodes = 2;\nnbasis = 3;\nnlags = 4;\nduration = 1000;\ndt = 1.0;\nbaseline = DiscreteHomogeneousProcess([1.0, 2.0], dt);\nweights = DenseWeightModel([0.1 0.2; 0.2 0.1]);\nimpulses = DiscreteGaussianImpulseResponse(ones(nnodes, nnodes, nbasis) ./ nbasis, nlags, dt);\nprocess = DiscreteStandardHawkesProcess(baseline, impulses, weights, dt);\ndata = rand(process, duration);\nres = mle!(process, data; verbose=true, regularize=false); # or mcmc!, vb!\nll = loglikelihood(process, data)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are several important differences to take note of. First, we now need to choose a time step size for the process, dt, which is also provided to all components that involve time (i.e., the baseline, impulse-response). Second, we use a discrete-time baseline process, DiscreteHomogeneousProcess, which is an exact discretized analog of HomogeneousProcess. NetworkHawkesProcesses also provides a discrete-time version of the non-homogeneous log Gaussian Cox process,  DiscreteLogGaussianCoxProcess. Third, we instantiate a discrete-time impulse response, DiscreteGaussianImpulseResponse, which has no continuous-time analog and is explained in detail below. Finally, data is now a Matrix{Int64} of size nnodes x duration containing event counts for each node along its rows. (Unlike the continuous-time model, we don't need to return nodes, events, and duration separately because they are implied by the data matrix dimensions).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Discrete-time Data)","category":"page"},{"location":"tutorial/#Discrete-time-Impulse-Responses","page":"Tutorial","title":"Discrete-time Impulse Responses","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Whereas continuous-time impulse responses are typically determined by probability density functions, discrete-time impulse responses are specified by probability mass functions (i.e., points on a simplex). The approach of NetworkHawkesProcesses is to construct such impulse responses as mixture models. That is, for a family of (scaled) basis distribution functions, phi_b  1 dots D  rightarrow 0 1 for b = 1 dots B, the impulse response is defined as","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hbard theta_n rightarrow n = sum_b=1^B theta_n rightarrow n^(b) cdot phi_bd \nsum_d=1^D phi_bd cdot Delta t = 1 \nsum_b=1^B theta_n rightarrow n^(b) = 1","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The discretized Gaussian family of basis functions is the only family currently provided by NetworkHawkesProcesses via the DiscreteGaussianImpulseResponse type. It's members are defined by","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"phi_bd = fracexp  -frac12 left( d - mu_b right)^2  Delta t cdot Z","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"where mu_b are evenly spaced on 1 D, sigma = fracDB - 1, and Z is a normalization constant. An example of one such distribution is shown below.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"(Image: Discrete Gaussian Impulse Response)","category":"page"}]
}
